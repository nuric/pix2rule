{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Generated DNF data\n",
    "This notebook collects and analyses experiment results that are run on the generated DNF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect imports\n",
    "from typing import Dict\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import utils.analysis\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5, linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup mlflow client\n",
    "mlclient = MlflowClient(tracking_uri=\"http://localhost:8888\") # Emptry string for local\n",
    "exp_list = [x.name for x in mlclient.list_experiments()]\n",
    "print(exp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Experiment Results\n",
    "We will look at aggregate metric performances of runs in a given experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore experiment and run data\n",
    "experiment_name = \"20200806-104823\"\n",
    "experiment_name = exp_list[-1]\n",
    "mlexp = mlclient.get_experiment_by_name(experiment_name)\n",
    "print(mlexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = utils.analysis.collect_experiment_data(experiment_name)\n",
    "exp_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trainint and test results\n",
    "df = exp_data.assign(relsgame_tasks=exp_data['relsgame_tasks'].map(lambda liststr: ','.join(sorted(eval(liststr)))))\n",
    "df = df[((df['learning_rate'] == 0.01) & (df['dnf_image_activation'] == 'tanh'))]\n",
    "display(df.groupby(['run_id']).count())\n",
    "fields = [k for k in df.columns if k.endswith('loss') and k != \"converged_loss\"]\n",
    "fields = [k for k in exp_data.columns if k.endswith('acc')]\n",
    "df = df.melt(id_vars=['epoch', 'relsgame_tasks', 'relsgame_train_size'], value_vars=fields)\n",
    "sns.relplot(x='epoch', y='value', hue='variable', kind='line', row='relsgame_tasks', col='relsgame_train_size', ci='sd', data=df)\n",
    "# Plot accuracy\n",
    "#fields = [k for k in exp_data.columns if k.endswith('acc')]\n",
    "#plot_data = exp_data.melt(id_vars=['step'], value_vars=fields)\n",
    "#sns.relplot(x='step', y='value', hue='variable', kind='line', data=plot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Run Report\n",
    "We can pick a single run and analyse the reports such as attention maps and the rules it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = exp_data.assign(relsgame_tasks=exp_data['relsgame_tasks'].map(lambda liststr: ','.join(sorted(eval(liststr)))))\n",
    "df[df['relsgame_tasks'] == '']['run_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather a specific run\n",
    "run_id = \"107801d186c54c6f8f8d431426b284c0\"\n",
    "#run_id = exp_data.groupby(by=\"run_id\").min().index[-1]\n",
    "print(\"Collecting artifacts for run:\", run_id)\n",
    "mlrun = mlclient.get_run(run_id)\n",
    "pprint(mlrun.to_dictionary())\n",
    "pprint(mlclient.list_artifacts(run_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_report(run_id: str, fpath: str):\n",
    "    \"\"\"Load numpy compressed report from given artifact.\"\"\"\n",
    "    local_path = mlclient.download_artifacts(run_id, fpath)\n",
    "    return np.load(local_path)\n",
    "\n",
    "report = load_report(run_id, \"train_report.npz\")\n",
    "print(report.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs=np.arange(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick some examples to analyse\n",
    "#labels = np.argmax(report['out_label'], -1)\n",
    "labels = report['out_label']\n",
    "predictions = np.argmax(report['prediction_label'], -1)\n",
    "idxs = np.flatnonzero(labels != predictions)[:8]\n",
    "#idxs = np.arange(8)\n",
    "print(idxs)\n",
    "print(np.stack([labels[idxs], predictions[idxs]]))\n",
    "print(report['prediction_label'][idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report['and_kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report['or_kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report['binary'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}